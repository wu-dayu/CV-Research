 
| **é¢œè‰²** | **ä»£è¡¨å«ä¹‰**       | **é€‚ç”¨åœºæ™¯**                                 |
| ------ | -------------- | ---------------------------------------- |
| **çº¢è‰²** | **æ ¸å¿ƒè´¡çŒ®/æ ¸å¿ƒç»“è®º**  | è®ºæ–‡è§£å†³çš„ç—›ç‚¹ã€Abstractå’ŒConclusionçš„å…³é”®å¥ã€‚         |
| **é»„è‰²** | **é‡è¦å®šä¹‰/æ¦‚å¿µ**    | ç¬¬ä¸€æ¬¡å‡ºç°çš„ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚ï¼šInductive Bias, Zero-shotï¼‰ã€‚ |
| **è“è‰²** | **æ•°å­¦å…¬å¼/ç†è®ºä¾æ®**  | æŸå¤±å‡½æ•° Loss Functionã€æ³¨æ„åŠ›æœºåˆ¶å…¬å¼ã€‚              |
| **ç»¿è‰²** | **å®éªŒç»“æœ/æ€§èƒ½æ•°æ®**  | SOTA è¡¨ç°ã€æ¶ˆèå®éªŒçš„å…³é”®æ•°æ®ã€‚                       |
| **ç´«è‰²** | **å€¼å¾—å€Ÿé‰´çš„ä»£ç /æ–¹æ³•** | å®ç°ç»†èŠ‚ï¼Œå¦‚â€œä½¿ç”¨äº† AdamW ä¼˜åŒ–å™¨â€ã€â€œå­¦ä¹ ç‡è¡°å‡ç­–ç•¥â€ã€‚         |
| **æ©™è‰²** | **ä¸è¶³/æœªæ¥å·¥ä½œ**    | ä½œè€…æ‰¿è®¤çš„é™åˆ¶ï¼ˆLimitationsï¼‰ï¼Œè¿™å¾€å¾€æ˜¯ä½ çš„é€‰é¢˜åˆ‡å…¥ç‚¹ã€‚        |
| **ç°è‰²** | **èƒŒæ™¯/å¼•ç”¨æ–‡çŒ®**    | ç»å…¸çš„å‚è€ƒæ–‡çŒ®ï¼Œæ ‡è®°ä»¥åè¦å»è¯»ã€‚                         |
| **é’è‰²** | **ä¸ªäººç–‘é—®/éšç¬”**    | è‡ªå·±è¯»ä¸æ‡‚çš„åœ°æ–¹ï¼Œå¾…æŸ¥é˜…èµ„æ–™æˆ–é—®å¯¼å¸ˆã€‚                      |
# ğŸ“„ [Paper Study] ViT---Vision Transformer 2026-01-19

## 1. å¿«é€Ÿé¢„è§ˆ (Quick Read - 10min)
- **å¹´ä»½/ä¼šåˆŠï¼š** {{2021}} / #ICLR #Arxiv 
- **é¢†åŸŸæ ‡ç­¾ï¼š** #CV/{{åº•å±‚}}
- **æ ¸å¿ƒç—›ç‚¹ (Motivation)ï¼š**
    - Transformeråœ¨computer visionä¸­çš„åº”ç”¨ååˆ†æœ‰é™
    - â€œIn vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place.â€ 
- **æ ¸å¿ƒè´¡çŒ® (Key Idea)ï¼š**
    - â€œWe show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.â€ 
    - Vision Transformer (ViT) å¯ä»¥å–å¾—å¾ˆå¥½çš„ç»“æœï¼ŒåŒæ—¶éœ€è¦çš„è®­ç»ƒè®¡ç®—èµ„æºæ¯”CNNæ›´å°‘
- **ä»£ç ä»“åº“ï¼š** [[GitHub Link](https://github.com/google-research/vision_transformer)]
- **æ˜¯å¦å€¼å¾—ç²¾è¯»ï¼š** ğŸŸ¢ å¿…è¯»

---

## 2. æ ¸å¿ƒæ¶æ„ä¸æ•°æ®æµ (Architecture & Data Flow)
> **æç¤ºï¼š** åˆ©ç”¨ Zotero çš„æˆªå›¾åŠŸèƒ½ï¼Œå°†æ¨¡å‹ç»“æ„å›¾ã€æµç¨‹å›¾è´´åœ¨æ­¤å¤„ã€‚

- **æ¨¡å‹ç»“æ„å›¾ï¼š**
  ![[Pasted image 20260119122733.png]]
- **å¼ é‡å˜åŒ– (Tensor Shapes - é‡ç‚¹)ï¼š**
    ![[f796881e1fc38b3c38033a109b088d40.jpg]]
    
---

## 3. æ·±åº¦æŠ€æœ¯æ£€æŸ¥æ¸…å• (Direction-Specific Checklist)

### ğŸŸ¢ åˆ†ç±»/éª¨å¹²ç½‘ç»œ (Classification/Backbone)
- [ ] **Basic Block:** æœ€å°é‡å¤å•å…ƒé•¿ä»€ä¹ˆæ ·ï¼Ÿ(æ®‹å·®ç»“æ„/Transformer Block/Ghost Block?)
- [ ] **ç‰¹å¾æå–:** å®ƒæ˜¯å¦‚ä½•æƒè¡¡å±€éƒ¨ç‰¹å¾ï¼ˆConvï¼‰å’Œå…¨å±€ç‰¹å¾ï¼ˆAttentionï¼‰çš„ï¼Ÿ
	Only focus on global characteristics
- [ ] **é™é‡‡æ ·:** å›¾åƒåˆ†è¾¨ç‡æ˜¯å¦‚ä½•ä¸€æ­¥æ­¥ç¼©å°çš„ï¼Ÿ(Stride Conv / Pooling / Patch Merging?)
- [ ] **æ€§èƒ½æŒ‡æ ‡:** å‚æ•°é‡ (Params) å’Œè®¡ç®—é‡ (FLOPs) å¤„äºä»€ä¹ˆé‡çº§ï¼Ÿ
- [ ] **Patch Size æ˜¯å¤šå°‘ï¼Ÿ** (è®ºæ–‡é»˜è®¤æ˜¯ 16x16ï¼Œè¿™å°±æ˜¯æ ‡é¢˜çš„ç”±æ¥)
	Depends on the model, for ViT-L it's 16 * 16
- [ ] **ä¸ºä»€ä¹ˆéœ€è¦ [CLS] Tokenï¼Ÿ** (æç¤ºï¼šå®ƒæ˜¯ä¸ºäº†æœ€ååšåˆ†ç±»ç”¨çš„â€œæ€»ç»“æ€§å‘é‡â€)
- [ ] **æ¨¡å‹åœ¨ä»€ä¹ˆæ•°æ®é›†ä¸Šè¡¨ç°å¥½ï¼Ÿ** (å…³æ³¨é¢„è®­ç»ƒæ•°æ®é›† JFT-300Mï¼Œç†è§£ä¸ºä»€ä¹ˆ ViT æ¯”è¾ƒâ€œåƒâ€æ•°æ®)
	ImageNet, CIFAR-100, VTAB, etc.
- [ ] **Inductive Bias (å½’çº³åç½®) æ˜¯ä»€ä¹ˆï¼Ÿ** (è¿™æ˜¯è®ºæ–‡çš„æ ¸å¿ƒè®ºç‚¹ï¼šCNN æœ‰å¹³ç§»ä¸å˜æ€§ç­‰å…ˆéªŒçŸ¥è¯†ï¼Œè€Œ Transformer æ²¡æœ‰ï¼Œæ‰€ä»¥éœ€è¦æ›´å¤šæ•°æ®å»å­¦ä¹ )
![[Pasted image 20260120135733.png]]
- â€œWe find that some heads attend to most of the image already in the lowest layers, showing that the ability to integrate information globally is indeed used by the model.â€ 

- â€œFurther, the attention distance increases with network depth. Globally, we find that the model attends to image regions that are semantically relevant for classificationâ€ 
ç›´è§‰ä¸Šè¯´CNNæ›´ç¬¦åˆç†è§£å›¾ç‰‡çš„ç›´è§‚ï¼Œå·ç§¯æ ¸æ‰«è¿‡ç›¸é‚»çš„å°åŒºåŸŸï¼Œå†æ£€æŸ¥æ€»ä½“å‘¨å›´çš„åŒºåŸŸï¼Œä¸äººç±»è§‚å¯Ÿå›¾ç‰‡çš„æ–¹æ³•ç±»ä¼¼ã€‚è€ŒTransformeråˆ™å…³æ³¨global chatacteristics.
---

## 4. æ•°å­¦è¡¨è¾¾ä¸ä»£ç å¤ç° (Math & Code)
- **æ ¸å¿ƒå…¬å¼ï¼š**
- ![[Pasted image 20260119122733.png]]
  ![[Pasted image 20260119144955.png]]
	**Eq.1: Patch Embedding:** 
		ç»™Transformer Encoderå‡†å¤‡è¾“å…¥åºåˆ—ï¼Œ$x_{class}$ å¯¹åº”0å·patch ([CLS] token), æ˜¯æ‰‹åŠ¨åŠ å…¥çš„ï¼Œä¸å¸¦å›¾ç‰‡ä¿¡æ¯çš„ç©ºå‘é‡ã€‚$x_p^n E$ æŠŠåˆ‡å¥½çš„æ¯ä¸€ä¸ªpatché€šè¿‡çŸ©é˜µEå˜æ¢æˆå‘é‡ã€‚$E_{pos}$æ˜¯ä½ç½®ç¼–ç ï¼Œé€šè¿‡æ·±åº¦å­¦ä¹ å¾—åˆ°ï¼Œèƒ½åˆ†è¾¨patchä¹‹é—´çš„å†…åœ¨å…³è”
		$z_0$ æ˜¯ä¸€ä¸ªN+1 by Dçš„çŸ©é˜µ è¾“å…¥Transformer Encoder.
		$x_{class}$ is a learned input, taken as classification.
	**Eq.2 MSA è‡ªæ³¨æ„åŠ›**, è¡¨ç°åœ¨Transformer Encoderç¤ºæ„å›¾çš„ä¸‹åŠéƒ¨åˆ† **è®¡ç®—Læ¬¡**
		[[LN]]: Layer Norm å½’ä¸€åŒ–ï¼Œè®©æ•°æ®åˆ†å¸ƒæ›´ç¨³å®šï¼Œé¿å…æ¢¯åº¦çˆ†ç‚¸
		[[MSA]]: Multi-Head Self Attention: æ¯ä¸ªå‘é‡å’Œå…¶ä»–å‘é‡è®¡ç®—å…³è”ï¼Œ0å·patchåœ¨æ­¤â€œå¸æ”¶â€å…¶ä»–patchçš„ä¿¡æ¯
		[[Residual Connection]]: $z_l-1$ æ®‹å·®é“¾æ¥ï¼Œæ–°ä¿¡æ¯ä¸æ—§ä¿¡æ¯å åŠ ï¼Œé¿å…ä¸¢å¤±ä¿¡æ¯ã€‚
	**Eq.3 MLP ç‰¹å¾è¿›åŒ–**ï¼Œ å˜ç°åœ¨ç¤ºæ„å›¾çš„ä¸ŠåŠéƒ¨åˆ†ï¼Œå½’ä¸€åŒ–åå¤šå±‚æ„ŸçŸ¥æœºï¼Œçº¿æ€§å˜æ¢çš„åŸºç¡€ä¸Šå¼•å…¥æ¿€æ´»å‡½æ•°ï¼Œæœ€åæ®‹å·®è¿æ¥[[Residual Connection]]ã€‚**è®¡ç®—Læ¬¡**
	**Eq.4 æœ€ç»ˆè¾“å‡º** å–å‡º $z_l^0$ è¿›è¡Œå½’ä¸€åŒ–åä½œä¸ºâ€œimage representation $y$ â€è¾“å‡º  
	**MLP Head**ï¼Œå°†Encoderçš„è¾“å‡ºä¸åˆ†ç±»çš„æ€»æ•°å¯¹åº”ï¼Œæ”¹å˜vector sizeã€‚ä¾‹å¦‚åˆ†5ç±»åˆ™768-->5. MLP Headåæ¥softmaxå®ç°åˆ†ç±»çš„æ¦‚ç‡è®¡ç®—ã€‚


- **ä»£ç æ ¸å¿ƒé€»è¾‘ (GitHub Snippets)ï¼š**
- Colab code PyTorch version: [Colab Link](https://colab.research.google.com/drive/13BRAMPXb3QG062Lzi4i5BJqTSgS8uXtj#scrollTo=TBZ3pQXMskB8)

---

## 5. æœ¬ç§‘ç”Ÿä¸“é¡¹ï¼šåŸºç¡€è¡¥è¯¾ä¸ç–‘é—® (To-Learn)

- [ ] **åŸºç¡€æ¦‚å¿µè¡¥è¯¾ (ç”¨ Obsidian åŒé“¾é“¾æ¥)ï¼š**  
    - [[Multihead Attention]] [[Attention]]-> æˆ‘çš„ç†è§£
	    - ![[Pasted image 20260121152120.png]]
	    - MHAå’ŒMHSAä¸­çš„å¯å­¦ä¹ å‚æ•°
		    - å°†è¾“å…¥æ˜ å°„åˆ°Query Key Valueç©ºé—´çš„ä¸‰ä¸ªçŸ©é˜µ, hä¸ºæ³¨æ„åŠ›å¤´æ•°$$W^Q\quad W^K\quad W^V \in (d_{model},d_{k}=d_{model}/h)$$
		    - è¾“å‡ºçŸ©é˜µ$$W^O\in (d_{model}, d_{model})$$$$Output=Concat(\sum_i head_i)W^O$$$$Output=\sum_{i=1}^h(Attention(Q_i,K_i,V_i)\cdot W_i^O)$$
			
- [ ] **é‡åˆ°çš„å‘ï¼š**
    
- [ ] å‡†å¤‡é—®å¸ˆå…„çš„é—®é¢˜ï¼š
    
    1. $E_{pos}$æ˜¯ä»€ä¹ˆ å–ä»£ç›é‡Œå¯»æ‰¾ç­”æ¡ˆ
---
## 6. å…³è”é˜…è¯»ä¸æ€»ç»“ (Summary & Links)

- **ä¸Šä¸€ä»£å·¥ä½œï¼š** 
    
- **æ ¸å¿ƒå¯¹æ¯”ï¼š** 
    

