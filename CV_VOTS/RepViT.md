
# ğŸ“„ [Paper Study] RepViT 2026-01-21

## 1. å¿«é€Ÿé¢„è§ˆ (Quick Read - 10min)
- **å¹´ä»½/ä¼šåˆŠï¼š** {{2024}} /  #CVPR #Arxiv
- **é¢†åŸŸæ ‡ç­¾ï¼š** #CV/{{åº•å±‚}} 
- **æ ¸å¿ƒç—›ç‚¹ (Motivation)ï¼š**
    - CNNä¸ViTä½œä¸ºä¸¤ä¸ªCVé¢†åŸŸç»å…¸çš„backboneï¼Œå½“å‰æ²¡æœ‰æœ‰æ•ˆçš„ç ”ç©¶å°†ViTçš„ç‰¹å¾å¸æ”¶è¿›å…¥CNNç½‘ç»œç»“æ„ä¸­ã€‚
- **æ ¸å¿ƒè´¡çŒ® (Key Idea)ï¼š**
    - è®ºæ–‡é€šè¿‡**ç»“æ„é‡å‚æ•°åŒ–ï¼ˆStructural Re-parameterizationï¼‰**ï¼Œå°† ViT çš„å…ˆè¿›å®è§‚æ¶æ„ï¼ˆå¦‚ Meta-Former ç»“æ„ï¼‰æ³¨å…¥åˆ°è½»é‡çº§ CNN ä¸­ï¼Œåœ¨ä¸å¢åŠ æ¨ç†æˆæœ¬çš„å‰æä¸‹æ˜¾è‘—æå‡æ€§èƒ½ã€‚å¦‚å›¾ï¼š
		    ![[Pasted image 20260124161430.png]]![[Pasted image 20260124161557.png]]
- **ä»£ç ä»“åº“ï¼š** [GitHub Link](https://github.com/THU-MIG/RepViT)
- **æ˜¯å¦å€¼å¾—ç²¾è¯»ï¼š** ğŸŸ¢ å¿…è¯» 

---

## 2. æ ¸å¿ƒæ¶æ„ä¸æ•°æ®æµ (Architecture & Data Flow)

- **Overall Architecture: RepViT Blocks in each stage 1:1:7:1**
	-![[Pasted image 20260122164937.png]]
	
- **RepViT Block Design**
	![[Pasted image 20260122165304.png]]
- **Macro Design**
	![[Pasted image 20260122165458.png]]
- **å¼ é‡å˜åŒ– (Tensor Shapes - é‡ç‚¹)ï¼š**
    ![[7bcdfa32407c89e9bbff47ed5b7a661f.jpg]]![[153e2ddb0d5293b3604d68b558a5fa60.jpg]]
    ![[99d43afec438e9e4cdeaaf95b8731253 2.jpg]]

---

## 3. æ·±åº¦æŠ€æœ¯æ£€æŸ¥æ¸…å• (Direction-Specific Checklist)

### ğŸŸ¢ åˆ†ç±»/éª¨å¹²ç½‘ç»œ (Classification/Backbone)
- [ ] **Basic Block:** æœ€å°é‡å¤å•å…ƒé•¿ä»€ä¹ˆæ ·ï¼Ÿ(æ®‹å·®ç»“æ„/Transformer Block/Ghost Block?)
- [ ] **ç‰¹å¾æå–:** å®ƒæ˜¯å¦‚ä½•æƒè¡¡å±€éƒ¨ç‰¹å¾ï¼ˆConvï¼‰å’Œå…¨å±€ç‰¹å¾ï¼ˆAttentionï¼‰çš„ï¼Ÿ
- [ ] **é™é‡‡æ ·:** å›¾åƒåˆ†è¾¨ç‡æ˜¯å¦‚ä½•ä¸€æ­¥æ­¥ç¼©å°çš„ï¼Ÿ(Stride Conv / Pooling / Patch Merging?)
- [ ] **æ€§èƒ½æŒ‡æ ‡:** å‚æ•°é‡ (Params) å’Œè®¡ç®—é‡ (FLOPs) å¤„äºä»€ä¹ˆé‡çº§ï¼Ÿ

---

## 4. æ•°å­¦è¡¨è¾¾ä¸ä»£ç å¤ç° (Math & Code)
- **ä»£ç æ ¸å¿ƒé€»è¾‘ (GitHub Snippets)ï¼š**
```
\\wsl$\Ubuntu\home\wudayu\CV_research\RepViT
```

---

## 5. æœ¬ç§‘ç”Ÿä¸“é¡¹ï¼šåŸºç¡€è¡¥è¯¾ä¸ç–‘é—® (To-Learn)

- [ ] **åŸºç¡€æ¦‚å¿µè¡¥è¯¾ (ç”¨ Obsidian åŒé“¾é“¾æ¥)ï¼š**
    - [[Teacher Model]] & [[Distillation]] 
	    ![[Pasted image 20260122155405.png]]
	    ![[Pasted image 20260122155454.png]]
	    â€œKnowledge distillation (Hinton et al., 2014) aims at reproducing the output of a large model with a smaller model by minimizing some distance between both outputs for a set of given inputs.â€ (Oquab ç­‰, 2024, p. 7)
    - [[Adam Optimizer]] & [[AdamW Optimizer]]
	    ![[Pasted image 20260122112643.png]]
	- [[æ·±åº¦å¯åˆ†ç¦»å·ç§¯]] i.e. [[Depthwise Separable Convolution]]
		å·ç§¯æ˜¯å¯¹åº”ä½ç½®å…ƒç´ ç›¸ä¹˜å¹¶æ±‚å’Œ
	  ![[00da3368ef918026e0163cd60e3946c6.jpg]]
	- [[FFN]] ä¸ [[Down Sampling]] ç­‰ç»´åº¦å˜æ¢çš„æ„ä¹‰
		![[Pasted image 20260122202029.png]]
- [ ] **é‡åˆ°çš„å‘ï¼š** 
	- Python 3.10 + CUDA 12.8 + PyTorch Nightly 
---

## 6. å…³è”é˜…è¯»ä¸æ€»ç»“ (Summary & Links)

- **ä¸Šä¸€ä»£å·¥ä½œï¼š**
    
- **æ ¸å¿ƒå¯¹æ¯”ï¼š** 
    

