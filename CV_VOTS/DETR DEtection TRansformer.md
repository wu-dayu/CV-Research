| **é¢œè‰²** | **ä»£è¡¨å«ä¹‰**       | **é€‚ç”¨åœºæ™¯**                                 |
| ------ | -------------- | ---------------------------------------- |
| **çº¢è‰²** | **æ ¸å¿ƒè´¡çŒ®/æ ¸å¿ƒç»“è®º**  | è®ºæ–‡è§£å†³çš„ç—›ç‚¹ã€Abstractå’ŒConclusionçš„å…³é”®å¥ã€‚         |
| **é»„è‰²** | **é‡è¦å®šä¹‰/æ¦‚å¿µ**    | ç¬¬ä¸€æ¬¡å‡ºç°çš„ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚ï¼šInductive Bias, Zero-shotï¼‰ã€‚ |
| **è“è‰²** | **æ•°å­¦å…¬å¼/ç†è®ºä¾æ®**  | æŸå¤±å‡½æ•° Loss Functionã€æ³¨æ„åŠ›æœºåˆ¶å…¬å¼ã€‚              |
| **ç»¿è‰²** | **å®éªŒç»“æœ/æ€§èƒ½æ•°æ®**  | SOTA è¡¨ç°ã€æ¶ˆèå®éªŒçš„å…³é”®æ•°æ®ã€‚                       |
| **ç´«è‰²** | **å€¼å¾—å€Ÿé‰´çš„ä»£ç /æ–¹æ³•** | å®ç°ç»†èŠ‚ï¼Œå¦‚â€œä½¿ç”¨äº† AdamW ä¼˜åŒ–å™¨â€ã€â€œå­¦ä¹ ç‡è¡°å‡ç­–ç•¥â€ã€‚         |
| **æ©™è‰²** | **ä¸è¶³/æœªæ¥å·¥ä½œ**    | ä½œè€…æ‰¿è®¤çš„é™åˆ¶ï¼ˆLimitationsï¼‰ï¼Œè¿™å¾€å¾€æ˜¯ä½ çš„é€‰é¢˜åˆ‡å…¥ç‚¹ã€‚        |
| **ç°è‰²** | **èƒŒæ™¯/å¼•ç”¨æ–‡çŒ®**    | ç»å…¸çš„å‚è€ƒæ–‡çŒ®ï¼Œæ ‡è®°ä»¥åè¦å»è¯»ã€‚                         |
| **é’è‰²** | **ä¸ªäººç–‘é—®/éšç¬”**    | è‡ªå·±è¯»ä¸æ‡‚çš„åœ°æ–¹ï¼Œå¾…æŸ¥é˜…èµ„æ–™æˆ–é—®å¯¼å¸ˆã€‚                      |
# ğŸ“„ [Paper Study] DETR 2026-01-30

## 1. å¿«é€Ÿé¢„è§ˆ (Quick Read - 10min)
- **å¹´ä»½/ä¼šåˆŠï¼š** {{2020}} /  #Arxiv
- **é¢†åŸŸæ ‡ç­¾ï¼š** #CV/{{æ£€æµ‹}} 
- **æ ¸å¿ƒç—›ç‚¹ (Motivation)ï¼š**
	å‰äººç ”ç©¶
    - ä¸æ˜¯çœŸæ­£çš„â€œEnd to Endâ€
    - å¯¹Anchoræåº¦ä¾èµ–
    - NMS éæå¤§å€¼æŠ‘åˆ¶ä¸å¯æˆ–ç¼º
    - ç¼ºä¹å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯
- **æ ¸å¿ƒè´¡çŒ® (Key Idea)ï¼š**
    - å¼•å…¥Transformeræ¶æ„å®ç°å…¨å±€å»ºæ¨¡
	    - Encoder åˆ©ç”¨è‡ªæ³¨æ„åŠ›å¯¹å…¨å›¾åƒç´ äº¤äº’ï¼Œè®©æ¯ä¸ªä½ç½®çš„ç‰¹å¾éƒ½æºå¸¦ä¸Šä¸‹æ–‡ä¿¡æ¯
	    - Decoder å¼•å…¥Object Query
	- æå‡ºBipartite Matching Loss äºŒåˆ†å›¾åŒ¹é…æŸå¤±
		- è¿™æ˜¯ DETR èƒ½å¤Ÿå»æ‰ NMS çš„æ ¹æºã€‚ä½œè€…å¼•å…¥äº†åŒˆç‰™åˆ©ç®—æ³•ï¼ˆHungarian Algorithmï¼‰ï¼Œå¼ºåˆ¶è¦æ±‚æ¨¡å‹ç”Ÿæˆçš„ $N$ ä¸ªé¢„æµ‹ç»“æœä¸å›¾åƒä¸­çš„ $M$ ä¸ªçœŸå®ç‰©ä½“è¿›è¡Œ**ä¸€å¯¹ä¸€åŒ¹é…**ã€‚
		- **ç‰©ç†æ„ä¹‰**ï¼šå¦‚æœä¸¤ä¸ª Query åŒæ—¶é¢„æµ‹äº†åŒä¸€ä¸ªç‰©ä½“ï¼Œå…¶ä¸­ä¸€ä¸ªä¼šè¢«åˆ¤å®šä¸ºâ€œæ­£ç¡®â€ï¼Œå¦ä¸€ä¸ªä¼šè¢«åˆ¤å®šä¸ºâ€œé”™è¯¯â€å¹¶å—åˆ°æƒ©ç½šã€‚è¿™è¿«ä½¿æ¨¡å‹å†…éƒ¨è‡ªå‘åœ°å­¦ä¼šäº†å»é‡ã€‚
	- ç«¯åˆ°ç«¯Pipeline
		- DETRè¾“å…¥æ˜¯å›¾åƒï¼Œè¾“å‡ºæ˜¯æ£€æµ‹æ¡†çš„é›†åˆï¼Œæ²¡æœ‰NMS/Anchor/Proposalç”Ÿæˆæ­¥éª¤
		- ç»Ÿä¸€çš„ä»»åŠ¡æ¡†æ¶ï¼šè¿™ç§åŸºäºé›†åˆé¢„æµ‹çš„è®¾è®¡ï¼Œä½¿å¾—æ¨¡å‹åªéœ€å¢åŠ ä¸€ä¸ªç®€å•çš„æ©ç åˆ†æ”¯ï¼Œå°±èƒ½ä»¥åŒæ ·é€»è¾‘å¤„ç†**å…¨æ™¯åˆ†å‰²ï¼ˆPanoptic Segmentationï¼‰**ä»»åŠ¡ã€‚
- **ä»£ç ä»“åº“ï¼š** [GitHub Link](https://github.com/facebookresearch/detr)
- **æ˜¯å¦å€¼å¾—ç²¾è¯»ï¼š** ğŸŸ¢ å¿…è¯» 

---

## 2. æ ¸å¿ƒæ¶æ„ä¸æ•°æ®æµ (Architecture & Data Flow)/å†…å®¹æ•´ç†
> **æç¤ºï¼š** åˆ©ç”¨ Zotero çš„æˆªå›¾åŠŸèƒ½ï¼Œå°†æ¨¡å‹ç»“æ„å›¾ã€æµç¨‹å›¾è´´åœ¨æ­¤å¤„ã€‚

- **æ¨¡å‹ç»“æ„å›¾ï¼š**
  ![[Pasted image 20260131130510.png]]![[Pasted image 20260131130638.png]]
	Encoderé‡å¤å…­æ¬¡ Decoderé‡å¤ï¼›**Object Querieså…¶å®æ˜¯learnable positional embedding**
   ![[Pasted image 20260131145215.png]]
   **å·¦ä¸‹æ–¹æ˜¯Object Queriesçš„è‡ªæ³¨æ„åŠ›æ“ä½œï¼ˆä¸»è¦ä¸ºäº†ç§»é™¤å†—ä½™æ¡†ï¼‰ï¼Œåœ¨ç¬¬ä¸€å±‚å¯ä»¥çœç•¥ï¼** ä½†åœ¨åé¢çš„å±‚ä¸èƒ½çœç•¥
   â€œThen, the decoder receives queries (initially set to zero), output positional encoding (object queries), and encoder memory, and produces the final set of predicted class labels and bounding boxes through multiple multihead self-attention and decoder-encoder attention. The first self-attention layer in the first decoder layer can be skipped.â€ (Carion ç­‰, 2020, p. 22)
   å…¨éƒ¨ç”¨äºé¢„æµ‹çš„FFNå…±äº«å‚æ•°
- Auxiliary decoding loss
	- â€œWe found helpful to use auxiliary losses [1] in decoder during training, especially to help the model output the correct number of objects of each class. We add prediction FFNs and Hungarian loss after each decoder layerâ€ (Carion ç­‰, 2020, p. 7)
- **å¼ é‡å˜åŒ– (Tensor Shapes - é‡ç‚¹)ï¼š**
    - [DETR è®ºæ–‡ç²¾è¯» bilibili video](https://www.bilibili.com/video/BV1GB4y1X72R/?share_source=copy_web&vd_source=0c211e12ad1b45ed8f807de27691bac9)
    - ![[Pasted image 20260201001829.png]]
    1Ã—91ï¼šCOCOçš„91ä¸ªç±»ï¼›1Ã—4ï¼šæ¡†çš„å››ä¸ªé¢„æµ‹å€¼

---

## 3. æ·±åº¦æŠ€æœ¯æ£€æŸ¥æ¸…å• (Direction-Specific Checklist)

### ğŸŸ¢ åˆ†ç±»/éª¨å¹²ç½‘ç»œ (Classification/Backbone)
- [ ] **Basic Block:** æœ€å°é‡å¤å•å…ƒé•¿ä»€ä¹ˆæ ·ï¼Ÿ(æ®‹å·®ç»“æ„/Transformer Block/Ghost Block?)
- [ ] **ç‰¹å¾æå–:** å®ƒæ˜¯å¦‚ä½•æƒè¡¡å±€éƒ¨ç‰¹å¾ï¼ˆConvï¼‰å’Œå…¨å±€ç‰¹å¾ï¼ˆAttentionï¼‰çš„ï¼Ÿ
- [ ] **é™é‡‡æ ·:** å›¾åƒåˆ†è¾¨ç‡æ˜¯å¦‚ä½•ä¸€æ­¥æ­¥ç¼©å°çš„ï¼Ÿ(Stride Conv / Pooling / Patch Merging?)
- [ ] **æ€§èƒ½æŒ‡æ ‡:** å‚æ•°é‡ (Params) å’Œè®¡ç®—é‡ (FLOPs) å¤„äºä»€ä¹ˆé‡çº§ï¼Ÿ

### ğŸ”µ ç›®æ ‡æ£€æµ‹ (Object Detection)
- [ ] **Neck & Head:** å®ƒæ˜¯å¦‚ä½•èåˆå¤šå°ºåº¦ç‰¹å¾çš„ï¼Ÿ(FPN, PAN, BiFPN ç­‰)
- [ ] **æ£€æµ‹èŒƒå¼:** æ˜¯é¢„è®¾å›ºå®šæ¡† (Anchor-based) è¿˜æ˜¯ç›´æ¥é¢„æµ‹ä¸­å¿ƒ (Anchor-free)ï¼Ÿ
	Anchor Free
- [ ] **æŸå¤±å‡½æ•°:** åˆ†ç±» Loss (å¦‚ Focal Loss) å’Œå›å½’ Loss (å¦‚ GIoU/DIoU) åˆ†åˆ«æ˜¯ä»€ä¹ˆï¼Ÿ
- [ ] **åå¤„ç†:** æ˜¯å¦éœ€è¦ NMS è¿‡æ»¤é‡å¤æ¡†ï¼Ÿè¿˜æ˜¯ End-to-end (å¦‚ DETR/RT-DETR)?
	End to End DETR

- **æ•°æ®å½’çº³åå·® (Inductive Bias)**ï¼šTransformer ç›¸æ¯” CNN ç¼ºå°‘å¹³ç§»ä¸å˜æ€§ï¼Œè¿™ç¯‡è®ºæ–‡æ˜¯å¦‚ä½•è¡¥å¿è¿™ä¸€ç‚¹çš„ï¼Ÿï¼ˆä¾‹å¦‚ï¼šPosition Encodingï¼‰
    
- **è®¡ç®—å¤æ‚åº¦**ï¼šè¯¥æ¨¡å‹çš„ç“¶é¢ˆæ˜¯åœ¨å›¾åƒç¼–ç å™¨ï¼ˆEncoderï¼‰è¿˜æ˜¯æç¤ºè§£ç å™¨ï¼ˆDecoderï¼‰ï¼Ÿè¿™å†³å®šäº†å®ƒåœ¨å®æ—¶éƒ¨ç½²æ—¶çš„è¡¨ç°ã€‚
    
- **æŸå¤±å‡½æ•°å‡½æ•°å½¢å¼**ï¼šä¸ºä»€ä¹ˆä½œè€…é€‰æ‹©äº†è¿™ä¸ªç‰¹å®šçš„ç»„åˆï¼ˆå¦‚ Dice Loss + Focal Lossï¼‰ï¼Ÿ

---

## 4. æ•°å­¦è¡¨è¾¾ä¸ä»£ç å¤ç° (Math & Code)
- **æ ¸å¿ƒå…¬å¼ï¼š**
	- **Matching Cost** å¯»æ‰¾æœ€ä¼˜åŒ¹é… äºŒåˆ†å›¾åŒ¹é…ä»£ä»· 
		- $$\hat{\sigma} = \arg \min_{\sigma \in \mathfrak{S}_N} \sum_{i}^{N} \mathcal{L}_{match}(y_i, \hat{y}_{\sigma(i)})$$
		- $\sigma(i)$ï¼šè¿™æ˜¯ä¸€ä¸ªæ’åˆ—ç»„åˆï¼Œä»£è¡¨ç¬¬ $i$ ä¸ªçœŸå®ç‰©ä½“è¢«æŒ‡æ´¾ç»™äº†ç¬¬ $\sigma(i)$ ä¸ªé¢„æµ‹ã€‚
		- $\mathcal{L}_{match}$ (åŒ¹é…ä»£ä»·)ï¼šæ³¨æ„ï¼Œè¿™ä¸æ˜¯æœ€ç»ˆçš„åå‘ä¼ æ’­ Lossï¼Œå®ƒåªæ˜¯ç”¨æ¥ç»™åŒˆç‰™åˆ©ç®—æ³•åšå‚è€ƒçš„â€œè·ç¦»åˆ†æ•°â€ã€‚  
		    $$\mathcal{L}_{match} = - \mathbb{1}_{\{c_i \neq \varnothing\}} \hat{p}_{\sigma(i)}(c_i) + \mathbb{1}_{\{c_i \neq \varnothing\}} \mathcal{L}_{box}(b_i, \hat{b}_{\sigma(i)})$$
		- å‚æ•°çš„ç‰©ç†æ„ä¹‰ï¼š
			- $\hat{p}_{\sigma(i)}(c_i)$ï¼šé¢„æµ‹ç»“æœä¸­ï¼Œå¯¹åº”çœŸå®ç±»åˆ« $c_i$ çš„æ¦‚ç‡å€¼ã€‚æ¦‚ç‡è¶Šé«˜ï¼Œä»£ä»·å€¼è¶Šä½ï¼Œè¶Šå€¾å‘äºåŒ¹é…
			- $\mathcal{L}_{box}$ï¼šé¢„æµ‹æ¡† $\hat{b}$ ä¸çœŸå®æ¡† $b$ çš„å‡ ä½•å·®å¼‚
			- $\mathbb{1}_{\{c_i \neq \varnothing\}}$ï¼šæŒ‡ç¤ºå‡½æ•°ã€‚æ„å‘³ç€æˆ‘ä»¬åªå¯¹â€œéèƒŒæ™¯â€çš„çœŸå®ç‰©ä½“å¯»æ‰¾åŒ¹é…ï¼Œå‰©ä¸‹çš„å…¨éƒ¨è‡ªåŠ¨å½’ä¸ºèƒŒæ™¯
	- **Hungarian Loss/ Prediction Loss**: åŒ¹é…å®Œæˆå è®¡ç®—çœŸæ­£çš„Losså‡½æ•°æ¥æ›´æ–°æƒé‡
		- $$\mathcal{L}_{H}(y, \hat{y}) = \sum_{i=1}^{N} \left[ -\log \hat{p}_{\hat{\sigma}(i)}(c_i) + \mathbb{1}_{\{c_i \neq \varnothing\}} \mathcal{L}_{box}(b_i, \hat{b}_{\hat{\sigma}(i)}) \right]$$
			- å¯¹è´Ÿå¯¹æ•°ä¼¼ç„¶$\log \hat p$, DETR å°†èƒŒæ™¯ç±»çš„æƒé‡ï¼ˆEmpty Weightï¼‰è®¾ä¸º **0.1**ï¼Œè€Œæœ‰ç‰©ä½“çš„ç±»åˆ«æƒé‡ä¸º **1**ã€‚è¿™ä¿è¯äº†æ¨¡å‹ä¸ä¼šä¸ºäº†å·æ‡’è€ŒæŠŠæ‰€æœ‰ä¸œè¥¿éƒ½é¢„æµ‹æˆèƒŒæ™¯ã€‚
		- $$\mathcal{L}_{box}(b_i, \hat{b}_{\sigma(i)}) = \lambda_{iou} \mathcal{L}_{iou}(b_i, \hat{b}_{\sigma(i)}) + \lambda_{L1} \|b_i - \hat{b}_{\sigma(i)}\|_1$$
			- $L_1$ Lossæä¾›ç¨³å®šçš„åæ ‡æ¼‚ç§»æ¢¯åº¦ï¼Œä½†å¯¹å°ºåº¦æ•æ„Ÿ
			- GIoU Lossè§£å†³å°ºåº¦é—®é¢˜
- **ä»£ç æ ¸å¿ƒé€»è¾‘ (GitHub Snippets)ï¼š**
	![[Pasted image 20260201003253.png]]

---

## 5. æœ¬ç§‘ç”Ÿä¸“é¡¹ï¼šåŸºç¡€è¡¥è¯¾ä¸ç–‘é—® (To-Learn)

- [ ] **åŸºç¡€æ¦‚å¿µè¡¥è¯¾ (ç”¨ Obsidian åŒé“¾é“¾æ¥)ï¼š**
    - 
- [ ] **é‡åˆ°çš„å‘ï¼š** 
    
- [ ] å‡†å¤‡é—®å¸ˆå…„çš„é—®é¢˜ï¼š
    **Decoderä¸­çš„Cross Attentionçš„åŸç†ï¼Œç»“åˆç»“æ„å›¾è§£é‡Š**
    Panoptic Segmentationä¸­çš„Tensorå˜åŒ–æƒ…å†µ
---

## 6. å…³è”é˜…è¯»ä¸æ€»ç»“ (Summary & Links)

- **ä¸Šä¸€ä»£å·¥ä½œï¼š** 
    
- **æ ¸å¿ƒå¯¹æ¯”ï¼š**
    
- **æˆ‘çš„è¯„ä»·ï¼š** è¿™ç¯‡è®ºæ–‡çš„æ€è·¯æ˜¯å¦å¯ä»¥å€Ÿé‰´åˆ°æˆ‘ç°åœ¨çš„å®éªŒä¸­ï¼Ÿ
    

