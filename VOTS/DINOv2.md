| **é¢œè‰²** | **ä»£è¡¨å«ä¹‰**       | **é€‚ç”¨åœºæ™¯**                                 |
| ------ | -------------- | ---------------------------------------- |
| **çº¢è‰²** | **æ ¸å¿ƒè´¡çŒ®/æ ¸å¿ƒç»“è®º**  | è®ºæ–‡è§£å†³çš„ç—›ç‚¹ã€Abstractå’ŒConclusionçš„å…³é”®å¥ã€‚         |
| **é»„è‰²** | **é‡è¦å®šä¹‰/æ¦‚å¿µ**    | ç¬¬ä¸€æ¬¡å‡ºç°çš„ä¸“ä¸šæœ¯è¯­ï¼ˆå¦‚ï¼šInductive Bias, Zero-shotï¼‰ã€‚ |
| **è“è‰²** | **æ•°å­¦å…¬å¼/ç†è®ºä¾æ®**  | æŸå¤±å‡½æ•° Loss Functionã€æ³¨æ„åŠ›æœºåˆ¶å…¬å¼ã€‚              |
| **ç»¿è‰²** | **å®éªŒç»“æœ/æ€§èƒ½æ•°æ®**  | SOTA è¡¨ç°ã€æ¶ˆèå®éªŒçš„å…³é”®æ•°æ®ã€‚                       |
| **ç´«è‰²** | **å€¼å¾—å€Ÿé‰´çš„ä»£ç /æ–¹æ³•** | å®ç°ç»†èŠ‚ï¼Œå¦‚â€œä½¿ç”¨äº† AdamW ä¼˜åŒ–å™¨â€ã€â€œå­¦ä¹ ç‡è¡°å‡ç­–ç•¥â€ã€‚         |
| **æ©™è‰²** | **ä¸è¶³/æœªæ¥å·¥ä½œ**    | ä½œè€…æ‰¿è®¤çš„é™åˆ¶ï¼ˆLimitationsï¼‰ï¼Œè¿™å¾€å¾€æ˜¯ä½ çš„é€‰é¢˜åˆ‡å…¥ç‚¹ã€‚        |
| **ç°è‰²** | **èƒŒæ™¯/å¼•ç”¨æ–‡çŒ®**    | ç»å…¸çš„å‚è€ƒæ–‡çŒ®ï¼Œæ ‡è®°ä»¥åè¦å»è¯»ã€‚                         |
| **é’è‰²** | **ä¸ªäººç–‘é—®/éšç¬”**    | è‡ªå·±è¯»ä¸æ‡‚çš„åœ°æ–¹ï¼Œå¾…æŸ¥é˜…èµ„æ–™æˆ–é—®å¯¼å¸ˆã€‚                      |
# ğŸ“„ [Paper Study] DINOv2 2026-01-25

## 1. å¿«é€Ÿé¢„è§ˆ (Quick Read - 10min)
- **å¹´ä»½/ä¼šåˆŠï¼š** {{2023}} / #Arxiv
- **é¢†åŸŸæ ‡ç­¾ï¼š** #CV/{{åº•å±‚}} 
- **æ ¸å¿ƒç—›ç‚¹ (Motivation)ï¼š**
    - å½“æ—¶çš„è§†è§‰æ¨¡å‹ä¾é â€œå›¾åƒ-æ–‡æœ¬å¯¹â€è¿›è¡Œç›‘ç£ï¼Œè™½ç„¶èƒ½è·å¾—å…¨å±€è¯­ä¹‰ï¼Œä½†ä¼šä¸¢å¤±åƒç´ çº§ç‰¹å¾ã€‚
    - å‰äººé‡‡ç”¨çš„è‡ªç›‘ç£æ–¹æ³•åœ¨å¤§æ•°æ®é›†è®­ç»ƒä¸Šæå…¶ä¸ç¨³å®šã€‚
    - å‰äººæ™®éè®¤ä¸ºè‡ªç›‘ç£å­¦ä¹ åªéœ€è¦å †æ•°æ®é‡ï¼ˆuncurated dataï¼‰ï¼Œä¸é‡è§†è´¨é‡ï¼Œå¯¼è‡´è®­ç»ƒæ•ˆæœå·®ã€‚
    - Meta çš„å›¢é˜Ÿå†™è¿™ç¯‡è®ºæ–‡æ˜¯ä¸ºäº†è¯æ˜ï¼š**è‡ªç›‘ç£å­¦ä¹ é€šè¿‡æ”¹è¿›â€œæ•°æ®ç²¾é€‰æµæ°´çº¿â€å’Œâ€œè®­ç»ƒç¨³å®šæ€§å·¥ç¨‹â€ï¼Œå¯ä»¥äº§ç”Ÿæ¯”å¼ºç›‘ç£æ¨¡å‹æ›´é€šç”¨ã€æ›´å¼ºå¤§ã€ä¸”åœ¨å›¾åƒçº§å’Œåƒç´ çº§ä»»åŠ¡ä¸Šå…¨é¢é¢†å…ˆçš„è§†è§‰ç‰¹å¾æå–å™¨ã€‚** ä»–ä»¬æƒ³æ‰“ç ´â€œåªæœ‰é æ–‡æœ¬æ ‡ç­¾æ‰èƒ½åšå¤§æ¨¡å‹â€çš„è¿·ä¿¡ï¼Œç¡®ç«‹è‡ªç›‘ç£å­¦ä¹ ä½œä¸º**è§†è§‰åŸºç¡€æ¨¡å‹ï¼ˆFoundation Modelsï¼‰çš„ç»Ÿæ²»åœ°ä½ã€‚
- **æ ¸å¿ƒè´¡çŒ® (Key Idea)ï¼š**
	- Distillation with No labels 
    - [Summary by Gemini](https://docs.google.com/document/d/1Eez54NqxIUQNmwWN47YwglgrthFHSBG2JgD-OktlYgE/edit?tab=t.0)
    - https://docs.google.com/document/d/1vIcf5c0YGtbBdzWcCtL_Huqd-nIFba1mnBLGx8lB3TA/edit?tab=t.0
- **ä»£ç ä»“åº“ï¼š** [GitHub Link](https://github.com/facebookresearch/dinov2)

---

## 2. æ ¸å¿ƒæ¶æ„ä¸æ•°æ®æµ (Architecture & Data Flow)
- **Data Processing æ•°æ®å¤„ç†**ï¼šcurated dataset
	![[Pasted image 20260125225958.png]]
	Uncurated Dataæ¥è‡ªç½‘ç»œï¼Œ
	- **Deduplication:** â€œWe apply the copy detection pipeline of Pizzi et al. (2022) to the uncurated data and remove near-duplicate images.â€ (Oquab ç­‰, 2024, p. 5)
	- **Retrieval:** â€œWe build our curated pretraining dataset by retrieving images from our uncurated data source that are close to images in our curated sources. In order to do this, we first compute an image embedding using a self-supervised ViT-H/16 network pretrained on ImageNet-22k, and use cosine-similarity as a distance measure between images.â€ (Oquab ç­‰, 2024, p. 5)
- **Discriminative Self-supervised Pre-training**
	- ä½¿ç”¨ä¼ ç»Ÿçš„ViTç»“æ„ï¼ŒModel Distillation **Teacher Modelçš„å‚æ•°**æ˜¯Studentå†å²å‚æ•°çš„åŠ¨é‡ç§»åŠ¨å¹³å‡(Teacher å‚æ•°=Student EMA)$$\theta_t^{(k)}=m\theta_t^{(k-1)}+(1-m)\theta_s^{(k-1)}$$
		- **è®­ç»ƒé˜¶æ®µ**ï¼šåŒæ¨¡å‹ï¼ˆTeacher + Studentï¼‰å¹¶è¡Œï¼ŒTeacher è´Ÿè´£äº§ç”Ÿç¨³å®šç›®æ ‡ï¼ŒStudent è´Ÿè´£æ¢¯åº¦æ›´æ–°ã€‚**student modelè§‚å¯Ÿè¢«maskedçš„å›¾åƒæˆ–cropsï¼Œteacher modelè§‚å¯Ÿå®Œæ•´çš„å›¾åƒå¹¶äº§ç”Ÿç›®æ ‡åˆ†å¸ƒ(Target Distribution)ï¼Œå¼•å¯¼studentåŒºæ¨¡ä»¿ã€‚**
		- **éƒ¨ç½²/æ¨ç†é˜¶æ®µ**ï¼šåªä¿ç•™**ä¸€ä¸ªæ¨¡å‹**ï¼ˆå³ Student/EMA æƒé‡ï¼‰ã€‚ä½ ä¸éœ€è¦å†è·‘ä¸¤ä¸ªç½‘ç»œï¼Œè¿™ä¿è¯äº†æ¨ç†æ—¶çš„é€Ÿåº¦å’Œ ViT åŸå§‹æ¶æ„å®Œå…¨ä¸€è‡´ã€‚
	- **æ ¸å¿ƒæŸå¤±å‡½æ•°**ï¼š DINO(ä¾§é‡å…¨å±€ç‰¹å¾)ä¸iBOT(ä¾§é‡å±€éƒ¨ç‰¹å¾)
		![[Pasted image 20260125231206.png]]
		- https://chatgpt.com/s/t_69763553fe5c8191a0257983b19631e9 ChatGPTå¯¹ä¸¤ç§Losså…¬å¼çš„è§£æï¼Œç›®å‰çœ‹ä¸æ‡‚
	- **å¤§è§„æ¨¡è®­ç»ƒçš„æŠ€æœ¯æ”¹è¿›**
		- DINOå’ŒiBOTä½¿ç”¨ä¸åŒçš„MLPæŠ•å½±å¤´
		- KoLeoæ­£åˆ™é¡¹ï¼Œâ€œencourages a uniform span of the features within a batchâ€ 
			- **ç‰©ç†æ„ä¹‰**ï¼šè¿™ä¸ªæ­£åˆ™é¡¹ä¼šè®¡ç®— batch å†…ç‰¹å¾ç‚¹ä¹‹é—´çš„æœ€å°è·ç¦»ï¼Œå¹¶é¼“åŠ±å®ƒä»¬**â€œæ•£å¼€â€** ã€‚å®ƒèƒ½ä¿ƒä½¿ç‰¹å¾åœ¨ç©ºé—´ä¸­å‘ˆå‡åŒ€åˆ†å¸ƒï¼Œé˜²æ­¢æ‰€æœ‰ç‰¹å¾èšåœ¨ä¸€èµ·ï¼Œä»è€Œæ˜¾è‘—æå‡äº†**å›¾åƒæ£€ç´¢**ä»»åŠ¡çš„æ€§èƒ½ ã€‚
		- å¤§è§„æ¨¡ä½¿ç”¨[[Stochastic Depth]] [[éšæœºæ·±åº¦]]ï¼Œé€šè¿‡éšæœºè·³è¿‡ä¸€äº›blockæ¥é˜²æ­¢è¿‡æ‹Ÿåˆå’Œè®­ç»ƒå´©æºƒ â€œThis saves memory and compute in proportion approximately equal to the drop rateâ€ 
---

## 3. æ·±åº¦æŠ€æœ¯æ£€æŸ¥æ¸…å• (Direction-Specific Checklist)

### ğŸŸ¢ åˆ†ç±»/éª¨å¹²ç½‘ç»œ (Classification/Backbone)
- [ ] **Basic Block:** æœ€å°é‡å¤å•å…ƒé•¿ä»€ä¹ˆæ ·ï¼Ÿ(æ®‹å·®ç»“æ„/Transformer Block/Ghost Block?)
- [ ] **ç‰¹å¾æå–:** å®ƒæ˜¯å¦‚ä½•æƒè¡¡å±€éƒ¨ç‰¹å¾ï¼ˆConvï¼‰å’Œå…¨å±€ç‰¹å¾ï¼ˆAttentionï¼‰çš„ï¼Ÿ
- [ ] **é™é‡‡æ ·:** å›¾åƒåˆ†è¾¨ç‡æ˜¯å¦‚ä½•ä¸€æ­¥æ­¥ç¼©å°çš„ï¼Ÿ(Stride Conv / Pooling / Patch Merging?)
- [ ] **æ€§èƒ½æŒ‡æ ‡:** å‚æ•°é‡ (Params) å’Œè®¡ç®—é‡ (FLOPs) å¤„äºä»€ä¹ˆé‡çº§ï¼Ÿ
---

## 4. æ•°å­¦è¡¨è¾¾ä¸ä»£ç å¤ç° (Math & Code)
- **æ ¸å¿ƒå…¬å¼ï¼š**
	  $$L_{total} = \lambda_1 L_{DINO} + \lambda_2 L_{iBOT}+\lambda_3 L_{Kleo}$$
- **ä»£ç æ ¸å¿ƒé€»è¾‘ (GitHub Snippets)ï¼š**
	/dinov2/layers
	/dinov2/loss
	/dinov2/models

---

## 5. æœ¬ç§‘ç”Ÿä¸“é¡¹ï¼šåŸºç¡€è¡¥è¯¾ä¸ç–‘é—® (To-Learn)

- [ ] **åŸºç¡€æ¦‚å¿µè¡¥è¯¾ (ç”¨ Obsidian åŒé“¾é“¾æ¥)ï¼š**
	- [è‡ªç›‘ç£å­¦ä¹ çš„ç‰¹å¾æŠ•å½±æœºåˆ¶](https://docs.google.com/document/d/190AH7nvk6jmrxfZ9nGKrI2dvHh-W5rjbeOtlY0dkMu0/edit?tab=t.0) By Gemini
		- é¢„è®­ç»ƒé˜¶æ®µï¼ˆæ—  Metadataï¼‰ï¼šæ¨¡å‹åœ¨åšâ€œæ‰¾å…±åŒç‚¹â€çš„æ¸¸æˆã€‚å®ƒæŠ•å½±çš„å¯¹è±¡æ˜¯åŒ¿åç‰¹å¾ç©ºé—´ï¼ˆPrototypesï¼‰ã€‚å®ƒä¸è¯†åˆ«è¯­ä¹‰ï¼Œåªè¯†åˆ«ç»“æ„å’Œçº¹ç†çš„ç›¸ä¼¼æ€§ã€‚
		- ä¸‹æ¸¸é˜¶æ®µï¼ˆå¾®è°ƒ/çº¿æ€§æ¢æµ‹ï¼‰ï¼šäººç±»ä»‹å…¥ï¼ŒæŠŠæ¨¡å‹è‡ªå‘å­¦åˆ°çš„â€œæ¨¡å¼â€ä¸â€œå•è¯ï¼ˆLabelsï¼‰â€å»ºç«‹æ˜ å°„å…³ç³»ã€‚
	- è®­ç»ƒæœ€å¼€å§‹å¦‚ä½•ä¿è¯å…¶æ”¶æ•› [è‡ªç›‘ç£å­¦ä¹ çš„æ”¶æ•›æœºåˆ¶](https://docs.google.com/document/d/1iG26S3eN9P2b2HY24EDAm3puQpwH_exGuHSXgH5fMtg/edit?tab=t.0)Inductive Bias; EMA; Anti-Collapse
	- **Sinkhorn-Knopp centering**å‘ç”Ÿåœ¨Teacher çš„è¾“å‡ºå±‚ (Heads)ï¼Œä¿è¯è®­ç»ƒç¨³å®šã€ä¸åç¼©ä¿è¯äº†åˆ†ç±»çš„å¤šæ ·æ€§
	- **KoLeo Regularizer**ä½œç”¨äºæœ€ç»ˆçš„ç‰¹å¾å‘é‡ (Embeddings)ï¼Œæå‡ç‰¹å¾çš„æ£€ç´¢æ€§èƒ½ã€ç²¾ç»†åº¦ï¼Œä¿è¯äº†ç‰¹å¾çš„åŒºåˆ†åº¦
	- [[Fine-tuning]]
		![[Pasted image 20260125163126.png]]![[Pasted image 20260125163155.png]]
	- [[Supervised Learning]]
		https://docs.google.com/document/d/1mVU8pfZTWFncAt0f4uLVKKtnLVU0N9p-vhP-iw_9YwI/edit?tab=t.0
	- [[Distillation]]
		â€œKnowledge distillation (Hinton et al., 2014) aims at reproducing the output of a large model with a smaller model by minimizing some distance between both outputs for a set of given inputs.â€ (Oquab ç­‰, 2024, p. 7)
	- [[EMA]]
		![[Pasted image 20260125230619.png]]
- [ ] **é‡åˆ°çš„å‘ï¼š**

---

## 6. å…³è”é˜…è¯»ä¸æ€»ç»“ (Summary & Links)

- **ä¸Šä¸€ä»£å·¥ä½œï¼š** [[ViT---Vision Transformer]] 
- **æ ¸å¿ƒå¯¹æ¯”ï¼š** 
	- Supervised vs Self-supervised: ViTçš„è®­ç»ƒä¾èµ–æ•°æ®æ ‡æ³¨ï¼Œæœ‰ç›‘ç£å­¦ä¹ ï¼Œè€ŒDINOv2ä¸ç„¶ã€‚å¯¼è‡´Losså‡½æ•°ä¸åŒã€‚DINOv2ä¸éœ€è¦ä»»ä½•æ ‡ç­¾ï¼Œå­¦åˆ°çš„ç‰¹å¾æ›´å…¨é¢ï¼Œæ›´å…·é€šç”¨æ€§ã€‚
	- è®­ç»ƒæ¶æ„ä¸åŒï¼Œå‰è€…æ²¡æœ‰self distillation